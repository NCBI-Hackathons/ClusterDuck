{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converts Abstract text into a list of terms\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    Use nlp techniques to process abstract text.\n",
    "    The case of the text is lowered and the punctuation is removed\n",
    "    The words are lemmatize\n",
    "    Keywords replaced with *keywords*\n",
    "    \"\"\"\n",
    "    def __init__(self, keywords, corpus):\n",
    "        self.keywords = keywords.lower()\n",
    "        self.corpus = corpus\n",
    "        self.abstracts = self.load()\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Read in corpus\n",
    "        \"\"\"\n",
    "        abstracts = pd.read_csv(self.corpus, index_col=0)\n",
    "        abstracts['AB'] = abstracts['AB'].str.lower()\n",
    "        abstracts['AB'] = abstracts['AB'].str.replace(self.keywords, '')\n",
    "        return abstracts\n",
    "    \n",
    "    def preprocess(self, abstract):\n",
    "        \"\"\"\n",
    "        These are the steps for normalizing the abstract text\n",
    "        Convert an abstract to word tokens.  This is done by tokenizing the text,\n",
    "        removing english stopwords and punctuation,and finally lemmatizing the words. \n",
    "        \n",
    "        Args:\n",
    "            abstract(str): Indivdual abstracts\n",
    "        \n",
    "        Return:\n",
    "            words(list): list of normalized words\n",
    "        \n",
    "        \"\"\"\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "        \n",
    "        # Instantiate Lemmanizer\n",
    "        WNL = WordNetLemmatizer()\n",
    "\n",
    "        # tokenize words, remove punctuation\n",
    "        tokenizer = RegexpTokenizer(r'\\w[\\w-]+')\n",
    "        tokens = tokenizer.tokenize(abstract)\n",
    "        # print(tokens)\n",
    "\n",
    "        # Remove stopwords and lemmatize tokens\n",
    "        words = [WNL.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
    "        words.append('*{}*'.format(self.keywords))\n",
    "        return words\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Entry point into Class\n",
    "        Generates list of terms in each document\n",
    "        \n",
    "        Args:\n",
    "            keywords(str): The keyword term from HPO phenotype\n",
    "            corpus(str): The path to corpus\n",
    "        \n",
    "        Returns:\n",
    "            documents(Pandas.Series.Series): Series of words in each document\n",
    "            \n",
    "        \"\"\"\n",
    "        documents = self.abstracts.AB.apply(self.preprocess)\n",
    "        return documents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [document, represents, official, position, ame...\n",
       "1      [background, randomised, controlled, trial, rc...\n",
       "2      [background, ketoacidosis, dka, serious, compl...\n",
       "3      [role, arsenic, trioxide, as2o3, inhibiting, i...\n",
       "4      [study, aimed, analyze, scientific, literature...\n",
       "5      [introduction, clear, unmet, clinical, need, p...\n",
       "6      [aim, compare, continuous, subcutaneous, insul...\n",
       "7      [background, objective, describe, frequency, c...\n",
       "8      [introduction, dyslipidemia, hyperglycemia, me...\n",
       "9      [introduction, nephropathy, leading, cause, mo...\n",
       "10     [objective, offspring, pregnancy, affected, ge...\n",
       "11     [background, prader-willi, syndrome, pws, ofte...\n",
       "12     [context, lipodystrophy, syndrome, rare, disor...\n",
       "13     [objective, decline, insulin, sensitivity, si,...\n",
       "14     [purpose, anti-mullerian, hormone, amh, propos...\n",
       "15     [worrisome, rise, pediatric, type, diabetes, t...\n",
       "16     [background, congenital, hyperinsulinism, chi,...\n",
       "17     [background, prospective, data, suggest, depre...\n",
       "18     [puberty, time, considerable, metabolic, hormo...\n",
       "19     [early, menarche, strongly, associated, adulth...\n",
       "20     [basic, helix-loop-helix, bhlh, transcription,...\n",
       "21     [two, decade, ago, cloning, autoimmune, regula...\n",
       "22     [background, infant, mother, shown, several, s...\n",
       "23     [background, diabetes, serious, chronic, disea...\n",
       "24     [background, rising, prevalence, childhood, ob...\n",
       "25     [context, multiple, consensus, statement, decr...\n",
       "26     [objective, estimate, incidence, type, diabete...\n",
       "27     [importance, early, exposure, complex, dietary...\n",
       "28     [4-methyl-2-, 2-methylbenzyl, amino, 3-thiazol...\n",
       "29     [background, united, state, largest, number, c...\n",
       "                             ...                        \n",
       "90     [basic, helix-loop-helix, bhlh, transcription,...\n",
       "91     [two, decade, ago, cloning, autoimmune, regula...\n",
       "92     [background, infant, mother, shown, several, s...\n",
       "93     [background, diabetes, serious, chronic, disea...\n",
       "94     [background, rising, prevalence, childhood, ob...\n",
       "95     [context, multiple, consensus, statement, decr...\n",
       "96     [objective, estimate, incidence, type, diabete...\n",
       "97     [importance, early, exposure, complex, dietary...\n",
       "98     [4-methyl-2-, 2-methylbenzyl, amino, 3-thiazol...\n",
       "99     [background, united, state, largest, number, c...\n",
       "100    [neonatal, diabetes, mellitus, ndm, monogenic,...\n",
       "101    [chromosome, abnormality, paternal, uniparenta...\n",
       "102    [diabetes, mellitus, associated, sensory, abno...\n",
       "103    [deficiency, excess, intake, trace, element, i...\n",
       "104    [neonatal, diabetes, mellitus, underdiagnosed,...\n",
       "105    [fanconi-bickel, syndrome, caused, mutation, s...\n",
       "106    [background, methylation, defect, chromosome, ...\n",
       "107    [plagl1, zac1, undergoes, parental, genomic, i...\n",
       "108    [imprinted, gene, plagl1, important, regulator...\n",
       "109    [background, neonatal, diabetes, mellitus, ndm...\n",
       "110    [ketoacidosis, dka, one, common, cause, morbid...\n",
       "111    [background, ketoacidosis, hyperosmolar, hyper...\n",
       "112    [background, ketoacidosis, dka, serious, compl...\n",
       "113    [introduction, clear, unmet, clinical, need, p...\n",
       "114    [background, objective, describe, frequency, c...\n",
       "115    [aim, identify, determinant, glycated, haemogl...\n",
       "116    [thrombotic, thrombocytopenic, purpura, ttp, s...\n",
       "117    [sodium-glucose, co-transporter, sglt2, inhibi...\n",
       "118    [glycogenic, hepatopathy, gh, rare, complicati...\n",
       "119    [emphysematous, osteomyelitis, rare, potential...\n",
       "Name: AB, Length: 120, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Preprocessor('diabetic', 'initial_corpus.csv')\n",
    "a.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
